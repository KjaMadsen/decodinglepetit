{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, val, Test splitter\n",
    "import train_pytorch\n",
    "import dataset_utils as utils\n",
    "import random\n",
    "import time \n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\"\n",
    "raw_data_dir = \"raw_data/derivatives/\"\n",
    "annotation_file = f\"raw_data/annotation/lpp{language}_word_information.csv\"\n",
    "random.seed(1234)\n",
    "\n",
    "pos = \"ADJ\" #Change this to select labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load the data with the selected configuration, only need to run this cell if \n",
    "#you haven't loaded the data before, or if you want to change config\n",
    "\n",
    "#Uncomment these two if you are changing config\n",
    "# utils.clear_data_dir()\n",
    "# shutil.rmtree(\"aux_data/\")\n",
    "\n",
    "utils.raw2npy(raw_data_dir, language=language)\n",
    "config_version = \"inter_subject\" # // \"inter_subject\"\n",
    "data_dir = \"data\"\n",
    "split = (0.33, 0.33, 0.33) #train, val, test\n",
    "pos = \"VERB\" #Part-Of-Speech\n",
    "utils.prepare_dataset(config_version, \n",
    "                        data_dir, \n",
    "                        annotation_file, \n",
    "                        split, \n",
    "                        language, \n",
    "                        pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the labels \n",
    "all_labels = []\n",
    "for i in [\"Train\", \"Test\", \"Val\"]:\n",
    "    if os.path.isdir(i):\n",
    "        for run in range(9):\n",
    "            path = os.path.join(i, f\"data/{run}/{language}/labels.txt\")\n",
    "            with open(path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for l in lines:\n",
    "                    all_labels.append(l.strip(\"\\n\"))\n",
    "\n",
    "\n",
    "import json\n",
    "most_common = Counter(all_labels).most_common()\n",
    "with open(\"labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_dict = json.load(f)\n",
    "\n",
    "label_dict = {v:k for k,v in label_dict.items()}\n",
    "\n",
    "most_common_lemmas = []\n",
    "for i in range(len(most_common)):\n",
    "    lbl = int(most_common[i][0])\n",
    "    most_common_lemmas.append((label_dict[lbl], most_common[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 15\n",
    "vocab = []\n",
    "off_set = 0 #how many to exclude from the most frequent words to stop overfitting\n",
    "label_stats = most_common_lemmas[off_set+1:top+off_set+1]\n",
    "for w, _ in label_stats: #top 15 words\n",
    "    vocab.append(w.strip())\n",
    "    \n",
    "print(vocab)\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_labels = utils.prepare_handpicked_labels(annotation_file, vocab)\n",
    "utils.create_label_files(curated_labels, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "#config = f\"{config_version}_{language}_top15{pos}\"\n",
    "config = f\"config2_{language}_top15{pos}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels.json\", \"r\") as f:\n",
    "    oov = json.load(f)[\"oov\"]\n",
    "\n",
    "train_pytorch.train(15, batch_size = batch_size, num_epochs = 10, language=language, config=config, model = \"2d\", weights=None, oov=oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_m2dcnn as model\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import lpp_Dataset\n",
    "batch_size = 50\n",
    "binary = False\n",
    "test_dataloader = DataLoader(lpp_Dataset(\"Test/data\", language=language, ignoreOOV=oov), batch_size=batch_size, shuffle=False)\n",
    "if binary:\n",
    "        nb_classes = 2\n",
    "else:\n",
    "    with open(\"labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        nb_classes = len(json.load(f).keys())\n",
    "      \n",
    "print(nb_classes)\n",
    "train_pytorch.test(model.M2DCNN(numClass=nb_classes, \n",
    "                                numFeatues=30880, \n",
    "                                DIMX=74, DIMY=90, DIMZ=73), \n",
    "                                config,\n",
    "                                f\"results/{config}_weights.pth\", \n",
    "                                nb_classes, \n",
    "                                test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
