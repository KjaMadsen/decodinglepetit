{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, val, Test splitter\n",
    "import train_pytorch\n",
    "import prepare_dataset\n",
    "import random\n",
    "import time \n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "#os.chdir(\"/Users/kjartan/Documents/CompCogSci3/\") \n",
    "os.chdir(r\"C:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"CN\"\n",
    "unsorted_data_dir = \"raw_data/derivatives/\"\n",
    "annotation_file = f\"raw_data/annotation/{language}/lpp{language}_word_information.csv\"\n",
    "random.seed(1234)\n",
    "\n",
    "pos = \"NUM\" #Change this to select labels\n",
    "oov = \"-1\" #no need to change this\n",
    "binary = False #ignore this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load the data with the selected configuration, only need to run this cell if \n",
    "#you haven't loaded the data before, or if you want to change config\n",
    "\n",
    "#Uncomment these two if you are changing config\n",
    "prepare_dataset.clear_data_dir()\n",
    "shutil.rmtree(\"aux_data/\")\n",
    "\n",
    "prepare_dataset.load_data(unsorted_data_dir, language=language)\n",
    "config_version = prepare_dataset.config2(\"data/\", split=(0.8,0.1,0.1), language=language)\n",
    "#config_version = prepare_dataset.config1(\"data/\", split=(0.8,0.1,0.1), language=languge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only need to run this after an experiment if you want to change \"pos\", then run all the following cells\n",
    "\n",
    "#Move data back from aux_data/\n",
    "for phase in os.listdir(\"aux_data/\"):\n",
    "    for run in os.listdir(f\"aux_data/{phase}\"):\n",
    "        path = f\"aux_data/{phase}/{run}/{language}/\"\n",
    "        for file in os.listdir(path):\n",
    "            file_path = f\"data/{phase}/{run}/{language}/{file}\"\n",
    "            shutil.move(f\"aux_data/{phase}/{run}/{language}/{file}\",file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prepare_dataset.prepare_labels(annotation_file, \"data/\", language, pos=pos, oov=oov)\n",
    "\n",
    "#binary = prepare_dataset.convert_to_binary_labels(\"data/\", oov, language) #Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the labels \n",
    "all_labels = []\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                path = f\"data/{i}/{run}/{language}/labels.txt\"\n",
    "                with open(path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for l in lines:\n",
    "                        all_labels.append(l.strip(\"\\n\"))\n",
    "\n",
    "from collections import Counter\n",
    "import codecs\n",
    "\n",
    "most_common = Counter(all_labels).most_common()\n",
    "label_dict = {}\n",
    "with codecs.open(\"label_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        k = i.split(\"=\")\n",
    "        label_dict[k[1].strip(\"\\n \")] = k[0]\n",
    "most_common_lemmas = []\n",
    "for i in range(len(most_common)):\n",
    "    lbl = most_common[i][0]\n",
    "    most_common_lemmas.append((label_dict[lbl], most_common[i][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab15 = []\n",
    "off_set = 1 #=2 for ADJ, =1 for the rest (it is to exclude the most frequent words which leads to overfitting)\n",
    "label_stats = most_common_lemmas[off_set+1:15+off_set+1]\n",
    "for w, _ in label_stats: #top 15 words\n",
    "    vocab15.append(w.strip())\n",
    "    \n",
    "print(vocab15)\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab15\n",
    "prepare_dataset.prepare_handpicked_labels(annotation_file, \"data/\", vocab, oov=oov, language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude data that corresponds to oov\n",
    "with codecs.open(\"label_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        k = i.split(\"=\")\n",
    "        if k[0].strip() == oov:\n",
    "            label_2_remove = int(k[1].strip(\"\\n \"))\n",
    "            break\n",
    "print(label_2_remove)\n",
    "labels_dict = {}\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                labels = np.loadtxt(f\"data/{i}/{run}/{language}/labels.txt\")\n",
    "                labels_dict[run] = labels\n",
    "\n",
    "exclusion_idxs = {k:[] for k in range(9)}\n",
    "\n",
    "for run in labels_dict.keys():\n",
    "    for idx, label in enumerate(labels_dict[run]):\n",
    "        if label==label_2_remove:\n",
    "            exclusion_idxs[run].append(idx)\n",
    "\n",
    "\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                path = f\"data/{i}/{run}/{language}/labels.txt\"\n",
    "                with open(path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                with open(path, \"w\") as f:\n",
    "                    for line in lines:\n",
    "                        if line.strip(\"\\n \") != str(label_2_remove):\n",
    "                            f.write(line)\n",
    "\n",
    "count = 0\n",
    "for phase in os.listdir(\"data/\"):\n",
    "    for run in range(9):\n",
    "        path = f\"data/{phase}/{run}/{language}/\"\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                continue\n",
    "            file_path = f\"data/{phase}/{run}/{language}/{file}\"\n",
    "            idx = file.split(\"_\")[-1][:-4]\n",
    "            if int(idx) in exclusion_idxs[run]:\n",
    "                count +=1 \n",
    "                dest = f\"aux_data/{phase}/{run}/{language}/\"\n",
    "                if not os.path.exists(dest):\n",
    "                    os.makedirs(dest)\n",
    "                shutil.move(file_path, f\"aux_data/{phase}/{run}/{language}/\")\n",
    "                #time.sleep(0.00001)\n",
    "print(\"moved \", count, \" files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_files():\n",
    "    counts = {}\n",
    "    # Iterate directory\n",
    "    for phase in os.listdir(\"data\"):\n",
    "        if os.path.isdir(\"data/\"+phase):\n",
    "            count=0\n",
    "            for run in os.listdir(f\"data/{phase}/\"):\n",
    "                for file in os.listdir(f\"data/{phase}/{run}/{language}\"):\n",
    "                    if file.endswith(\".npy\"):\n",
    "                        count += 1\n",
    "        counts[phase] = count\n",
    "    return counts\n",
    "\n",
    "count_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline(lbl = \"data/Test/\", language = \"EN\"):\n",
    "    prob = 0\n",
    "    labels = []\n",
    "    for run in os.listdir(lbl):\n",
    "        path = lbl +\"/\"+ run + \"/\" + language\n",
    "        multiplier = len(os.listdir(path))-1\n",
    "        if multiplier>0:\n",
    "            for _ in range(int(multiplier/len(np.loadtxt(path+\"/labels.txt\")))):\n",
    "                labels.append(np.loadtxt(path+\"/labels.txt\"))\n",
    "    labels = np.concatenate(labels).ravel()\n",
    "    total = labels.size\n",
    "    label_c = Counter(labels)\n",
    "    for _, v in label_c.items():\n",
    "        prob += (v/total)**2\n",
    "    return prob\n",
    "\n",
    "calc_baseline(language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a log\n",
    "import json\n",
    "import datetime\n",
    "with codecs.open(\"config_stats.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()}\\n\")\n",
    "    f.write(f\"{off_set+1}th to {off_set+15}th\\n\")\n",
    "    for i in label_stats:\n",
    "        f.write(str(i) + \", \")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(json.dumps(count_files()))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "#config = f\"{config_version}_{language}_top15{pos}\"\n",
    "config = f\"config2_{language}_top15{pos}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pytorch.train(binary=False, batch_size = batch_size, num_epochs = 10, language=language, config=config, model = \"2d\", weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_m2dcnn as model\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import lpp_Dataset\n",
    "\n",
    "batch_size = 50\n",
    "binary = False\n",
    "test_dataloader = DataLoader(lpp_Dataset(\"data/Test/\"), batch_size=batch_size, shuffle=False)\n",
    "if binary:\n",
    "        nb_classes = 2\n",
    "else:\n",
    "    with open(\"label_dict.txt\", \"r\") as f:\n",
    "        nb_classes = len(f.readlines())\n",
    "      \n",
    "print(nb_classes)\n",
    "train_pytorch.test(model.M2DCNN(numClass=nb_classes, numFeatues=30880, DIMX=74, DIMY=90, DIMZ=73), config,f\"results/{config}_weights.pth\", nb_classes, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogSciEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee8826d4702f16d0fb1dbf01a1d644cd38b1dd67f0d2287bca04130bed3d8f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
