{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, val, Test splitter\n",
    "import train_pytorch\n",
    "import prepare_dataset\n",
    "import random\n",
    "import time \n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "#os.chdir(\"/Users/kjartan/Documents/CompCogSci3/\") \n",
    "os.chdir(r\"C:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"CN\"\n",
    "unsorted_data_dir = \"raw_data/derivatives/\"\n",
    "annotation_file = f\"raw_data/annotation/{language}/lpp{language}_word_information.csv\"\n",
    "random.seed(1234)\n",
    "\n",
    "pos = \"VERB\" #Change this to select labels\n",
    "oov = \"-1\" #no need to change this\n",
    "binary = False #ignore this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load the data with the selected configuration, only need to run this cell if \n",
    "#you haven't loaded the data before, or if you want to change config\n",
    "\n",
    "#Uncomment these two if you are changing config\n",
    "#prepare_dataset.clear_data_dir()\n",
    "#shutil.rmtree(\"aux_data/\")\n",
    "\n",
    "#prepare_dataset.load_data(unsorted_data_dir, language=language)\n",
    "config_version = prepare_dataset.config2(\"data/\", split=(0.8,0.1,0.1), language=language)\n",
    "#config_version = prepare_dataset.config1(\"data/\", split=(0.8,0.1,0.1), language=languge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only need to run this after an experiment if you want to change \"pos\", then run all the following cells\n",
    "\n",
    "#Move data back from aux_data/\n",
    "for phase in os.listdir(\"aux_data/\"):\n",
    "    for run in os.listdir(f\"aux_data/{phase}\"):\n",
    "        path = f\"aux_data/{phase}/{run}/{language}/\"\n",
    "        for file in os.listdir(path):\n",
    "            file_path = f\"data/{phase}/{run}/{language}/{file}\"\n",
    "            shutil.move(f\"aux_data/{phase}/{run}/{language}/{file}\",file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prepare_dataset.prepare_labels(annotation_file, \"data/\", language, pos=pos, oov=oov)\n",
    "\n",
    "#binary = prepare_dataset.convert_to_binary_labels(\"data/\", oov, language) #Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the labels \n",
    "all_labels = []\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                path = f\"data/{i}/{run}/{language}/labels.txt\"\n",
    "                with open(path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for l in lines:\n",
    "                        all_labels.append(l.strip(\"\\n\"))\n",
    "\n",
    "from collections import Counter\n",
    "import codecs\n",
    "\n",
    "most_common = Counter(all_labels).most_common()\n",
    "label_dict = {}\n",
    "with codecs.open(\"label_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        k = i.split(\"=\")\n",
    "        label_dict[k[1].strip(\"\\n \")] = k[0]\n",
    "most_common_lemmas = []\n",
    "for i in range(len(most_common)):\n",
    "    lbl = most_common[i][0]\n",
    "    most_common_lemmas.append((label_dict[lbl], most_common[i][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab15 = []\n",
    "off_set = 0 #=2 for ADJ, =1 for the rest (it is to exclude the most frequent words which leads to overfitting)\n",
    "label_stats = most_common_lemmas[off_set+1:15+off_set+1]\n",
    "for w, _ in label_stats: #top 15 words\n",
    "    vocab15.append(w.strip())\n",
    "    \n",
    "print(vocab15)\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab15\n",
    "prepare_dataset.prepare_handpicked_labels(annotation_file, \"data/\", vocab, oov=oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude data that corresponds to oov\n",
    "with codecs.open(\"label_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in f.readlines():\n",
    "        k = i.split(\"=\")\n",
    "        if k[0].strip() == oov:\n",
    "            label_2_remove = int(k[1].strip(\"\\n \"))\n",
    "            break\n",
    "print(label_2_remove)\n",
    "labels_dict = {}\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                labels = np.loadtxt(f\"data/{i}/{run}/{language}/labels.txt\")\n",
    "                labels_dict[run] = labels\n",
    "\n",
    "exclusion_idxs = {k:[] for k in range(9)}\n",
    "\n",
    "for run in labels_dict.keys():\n",
    "    for idx, label in enumerate(labels_dict[run]):\n",
    "        if label==label_2_remove:\n",
    "            exclusion_idxs[run].append(idx)\n",
    "\n",
    "\n",
    "for i in os.listdir(\"data\"):\n",
    "    if os.path.isdir(\"data/\"+i):\n",
    "        for phase in os.listdir(\"data/\"+i):\n",
    "            for run in range(9):\n",
    "                path = f\"data/{i}/{run}/{language}/labels.txt\"\n",
    "                with open(path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                with open(path, \"w\") as f:\n",
    "                    for line in lines:\n",
    "                        if line.strip(\"\\n \") != str(label_2_remove):\n",
    "                            f.write(line)\n",
    "\n",
    "count = 0\n",
    "for phase in os.listdir(\"data/\"):\n",
    "    for run in range(9):\n",
    "        path = f\"data/{phase}/{run}/{language}/\"\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                continue\n",
    "            file_path = f\"data/{phase}/{run}/{language}/{file}\"\n",
    "            idx = file.split(\"_\")[-1][:-4]\n",
    "            if int(idx) in exclusion_idxs[run]:\n",
    "                count +=1 \n",
    "                dest = f\"aux_data/{phase}/{run}/{language}/\"\n",
    "                if not os.path.exists(dest):\n",
    "                    os.makedirs(dest)\n",
    "                shutil.move(file_path, f\"aux_data/{phase}/{run}/{language}/\")\n",
    "                #time.sleep(0.00001)\n",
    "print(\"moved \", count, \" files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_files():\n",
    "    counts = {}\n",
    "    # Iterate directory\n",
    "    for phase in os.listdir(\"data\"):\n",
    "        if os.path.isdir(\"data/\"+phase):\n",
    "            count=0\n",
    "            for run in os.listdir(f\"data/{phase}/\"):\n",
    "                for file in os.listdir(f\"data/{phase}/{run}/{language}\"):\n",
    "                    if file.endswith(\".npy\"):\n",
    "                        count += 1\n",
    "        counts[phase] = count\n",
    "    return counts\n",
    "\n",
    "count_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_baseline(lbl = \"data/Test/\", language = \"EN\"):\n",
    "    prob = 0\n",
    "    labels = []\n",
    "    for run in os.listdir(lbl):\n",
    "        path = lbl +\"/\"+ run + \"/\" + language\n",
    "        multiplier = len(os.listdir(path))-1\n",
    "        if multiplier>0:\n",
    "            for _ in range(int(multiplier/len(np.loadtxt(path+\"/labels.txt\")))):\n",
    "                labels.append(np.loadtxt(path+\"/labels.txt\"))\n",
    "    labels = np.concatenate(labels).ravel()\n",
    "    total = labels.size\n",
    "    label_c = Counter(labels)\n",
    "    for _, v in label_c.items():\n",
    "        prob += (v/total)**2\n",
    "    return prob\n",
    "\n",
    "calc_baseline(language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a log\n",
    "import json\n",
    "import datetime\n",
    "with codecs.open(\"config_stats.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"{datetime.datetime.now()}\\n\")\n",
    "    f.write(f\"{off_set+1}th to {off_set+15}th\\n\")\n",
    "    for i in label_stats:\n",
    "        f.write(str(i) + \", \")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(json.dumps(count_files()))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "config = f\"{config_version}_{language}_top15{pos}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-12 21:46:28.015507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\\mt_deep-master\\scripts\\model_m2dcnn.py:78: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight, gain=nn.init.calculate_gain('relu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/10\n",
      "-------------\n",
      "{'train': <torch.utils.data.dataloader.DataLoader object at 0x0000015352E1B550>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x0000015352E1B4C0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x0000015352E1AAA0>}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_pytorch\u001b[39m.\u001b[39;49mtrain(binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, batch_size \u001b[39m=\u001b[39;49m batch_size, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, language\u001b[39m=\u001b[39;49mlanguage, config\u001b[39m=\u001b[39;49mconfig, model \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m2d\u001b[39;49m\u001b[39m\"\u001b[39;49m, weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\\mt_deep-master\\scripts\\train_pytorch.py:294\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(binary, batch_size, num_epochs, language, config, model, weights)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mprint\u001b[39m(t0)\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2d\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 294\u001b[0m     train_m2dcnn(path, config, language, nb_classes\u001b[39m=\u001b[39;49mnb_classes, batch_size\u001b[39m=\u001b[39;49mbatch_size, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, weights\u001b[39m=\u001b[39;49mweights)\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     train_3dcnn(path, config, num_epochs\u001b[39m=\u001b[39mnum_epochs, batch_size\u001b[39m=\u001b[39mbatch_size, nb_classes\u001b[39m=\u001b[39mnb_classes)\n",
      "File \u001b[1;32mc:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\\mt_deep-master\\scripts\\train_pytorch.py:174\u001b[0m, in \u001b[0;36mtrain_m2dcnn\u001b[1;34m(dataset_path, condition, langauge, nb_classes, batch_size, num_epochs, weights)\u001b[0m\n\u001b[0;32m    171\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,betas\u001b[39m=\u001b[39m(\u001b[39m0.9\u001b[39m, \u001b[39m0.999\u001b[39m))\n\u001b[0;32m    172\u001b[0m scheduler \u001b[39m=\u001b[39m ExponentialLR(optimizer, gamma\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m)\n\u001b[1;32m--> 174\u001b[0m model, train_accuracy, valid_accuracy \u001b[39m=\u001b[39m train_model_m2dcnn(model, dataloaders_dict, criterion,\n\u001b[0;32m    175\u001b[0m                                                     optimizer, scheduler, num_epochs \u001b[39m=\u001b[39;49m num_epochs)\n\u001b[0;32m    176\u001b[0m model, test_accuracy \u001b[39m=\u001b[39m test_model(model, dataloaders_dict, nb_classes)\n\u001b[0;32m    178\u001b[0m plot_loss_accuracy(train_accuracy, valid_accuracy, test_accuracy, condition)\n",
      "File \u001b[1;32mc:\\Users\\kjart\\OneDrive\\Dokumenter\\KU\\3. semester\\Cog sci 3\\CompCogSci3\\mt_deep-master\\scripts\\train_pytorch.py:75\u001b[0m, in \u001b[0;36mtrain_model_m2dcnn\u001b[1;34m(model, dataloaders_dict, criterion, optimizer, scheduler, num_epochs, is3D, schedule)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m# Backprop\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     76\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     78\u001b[0m batch_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)  \n",
      "File \u001b[1;32mc:\\Users\\kjart\\anaconda3\\envs\\cogSciEnv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kjart\\anaconda3\\envs\\cogSciEnv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "\n",
    "train_pytorch.train(binary=False, batch_size = batch_size, num_epochs = 10, language=language, config=config, model = \"2d\", weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_m2dcnn as model\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import lpp_Dataset\n",
    "\n",
    "batch_size = 50\n",
    "binary = False\n",
    "test_dataloader = DataLoader(lpp_Dataset(\"data/Test/\"), batch_size=batch_size, shuffle=False)\n",
    "if binary:\n",
    "        nb_classes = 2\n",
    "else:\n",
    "    with open(\"label_dict.txt\", \"r\") as f:\n",
    "        nb_classes = len(f.readlines())\n",
    "      \n",
    "print(nb_classes)\n",
    "train_pytorch.test(model.M2DCNN(numClass=nb_classes, numFeatues=30880, DIMX=74, DIMY=90, DIMZ=73), config,f\"results/{config}_weights.pth\", nb_classes, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogSciEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee8826d4702f16d0fb1dbf01a1d644cd38b1dd67f0d2287bca04130bed3d8f05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
